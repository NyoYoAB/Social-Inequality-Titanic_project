{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset - Social Inequality Analysis\n",
    "\n",
    "## 1. Introduction\n",
    "This notebook explores the **social inequality** on the Titanic dataset by examining how passenger class, gender, and other factors influenced survival rates. We applied various machine learning models, including a **Shallow Artificial Neural Network (ANN)** and **Multiclass Logistic Regression**, to predict survival based on key features.\n",
    "\n",
    "## 2. Data Preparation and Feature Engineering\n",
    "Before building machine learning models, we prepared the dataset by:\n",
    "- Handling duplicates : **95 duplicates has been removed** \n",
    "- Feature engineering `Pclass` by combining the one-hot encoded columns into a single categorical feature, preserving the ordinal nature of passenger class.\n",
    "\n",
    "### 2.1 Feature Engineering for `Pclass`\n",
    "- Initially, `Pclass` was one-hot encoded to `Pclass_1`, `Pclass_2`, `Pclass_3`, which resulted in an accuracy of **58%** with the logistic regression model.\n",
    "- After feature engineering `Pclass` by combining it into a single column, the accuracy significantly improved to **95.6%** with the logistic regression model.\n",
    "\n",
    "## 3. Model 1: Shallow Artificial Neural Network (ANN)\n",
    "\n",
    "### 3.1 Model Description\n",
    "We implemented a **Shallow ANN** with the following architecture:\n",
    "- **Input layer**: The number of input features.\n",
    "- **Hidden layer**: 6 neurons with the `tanh` activation function.\n",
    "- **Output layer**: The softmax function to handle multiclass classification.\n",
    "\n",
    "### 3.2 Results\n",
    "- After training the model for 30,000 epochs, the **Shallow ANN** achieved an accuracy of **69.38%** (`np.float64(0.69375)`).\n",
    "\n",
    "### 3.3 Observations\n",
    "- The shallow ANN performed decently, but the accuracy was lower compared to the logistic regression model after feature engineering. This suggests that logistic regression is more effective for this dataset when `Pclass` is feature-engineered, potentially due to the smaller size and simplicity of the Titanic dataset.\n",
    "\n",
    "## 4. Model 2: Multiclass Logistic Regression\n",
    "\n",
    "### 4.1 Model Description\n",
    "We implemented a **Multiclass Logistic Regression** using softmax activation to handle the classification task. We focused on tuning the model by:\n",
    "- Feature engineering the `Pclass` feature.\n",
    "- Fine-tuning hyperparameters such as the learning rate (`eta`) and the number of epochs.\n",
    "\n",
    "### 4.2 Results\n",
    "- Without feature engineering the `Pclass` feature (using one-hot encoding), the logistic regression model achieved an accuracy of **58%**.\n",
    "- After feature engineering `Pclass`, the accuracy increased significantly to **95.6%**.\n",
    "\n",
    "### 4.3 Observations\n",
    "- The logistic regression model outperformed the shallow ANN when the `Pclass` feature was combined into a single column, highlighting the importance of proper feature engineering for this dataset.\n",
    "- The accuracy of 95.6% demonstrates that logistic regression can effectively model survival on the Titanic, especially with the engineered features.\n",
    "\n",
    "## 5. Hyperparameter Tuning and Performance Comparison\n",
    "\n",
    "### 5.1 Shallow ANN\n",
    "We performed hyperparameter tuning for the shallow ANN, experimenting with:\n",
    "- **Learning rates**: 1e-4, 1e-3, 1e-2.\n",
    "- **Number of neurons** in the hidden layer: 3, 6, 10.\n",
    "- **Number of epochs**: 5000, 10000, 30000.\n",
    "\n",
    "Despite extensive tuning, the shallow ANN achieved a maximum accuracy of **69.38%**, suggesting that more complex neural networks may be unnecessary for this dataset.\n",
    "\n",
    "### 5.2 Logistic Regression\n",
    "We tuned the logistic regression model and found that after feature engineering, it achieved a near-perfect accuracy of **95.6%**. This highlights the effectiveness of logistic regression for this classification task, especially with properly engineered features like `Pclass`.\n",
    "\n",
    "## 6. Conclusion\n",
    "- The **Multiclass Logistic Regression** model performed best with an accuracy of **95.6%** after feature engineering the `Pclass` feature. This demonstrates the importance of preserving the ordinal relationship between passenger classes in the dataset.\n",
    "- The **Shallow ANN**, even with hyperparameter tuning, reached an accuracy of **69.38%**, which is lower compared to logistic regression.\n",
    "- Feature engineering and hyperparameter tuning played a critical role in improving model performance, particularly in the logistic regression model.\n",
    "\n",
    "## 7. Future Improvements\n",
    "- Consider using deeper neural networks or ensemble methods (e.g., Random Forest or Gradient Boosting) for more complex feature interactions.\n",
    "- Apply additional feature engineering techniques, such as binning continuous variables like `Age` and `Fare`, to further improve model performance.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
