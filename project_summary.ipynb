{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset - Social Inequality Analysis\n",
    "\n",
    "## 1. Introduction\n",
    "This notebook explores **social inequality** in the Titanic dataset by examining how passenger class, gender, and other factors influenced survival rates. We applied two machine learning models, a **Shallow Artificial Neural Network (ANN)** and **Multiclass Logistic Regression**, to predict survival based on key features.\n",
    "\n",
    "## 2. Data Preparation and Feature Engineering\n",
    "Before building machine learning models, we performed several data preparation steps:\n",
    "- **Handling duplicates**: We identified and removed **95 duplicates** from the dataset.\n",
    "- **Feature engineering `Pclass`**: We combined the one-hot encoded columns into a single categorical feature, preserving the ordinal nature of passenger class.\n",
    "\n",
    "### 2.1 Feature Engineering for `Pclass`\n",
    "- Initially, `Pclass` was one-hot encoded into `Pclass_1`, `Pclass_2`, `Pclass_3`, which resulted in an accuracy of **58%** with the logistic regression model.\n",
    "- After feature engineering `Pclass` by combining it into a single column, the accuracy significantly improved to **95.6%** with the logistic regression model.\n",
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 3.1 Survival Rate Distribution Across Classes and Genders\n",
    "- A bar plot of survival rates across different passenger classes and genders shows a clear pattern:\n",
    "    - **First-class passengers** had the highest survival rate, with females surviving at a much higher rate than males.\n",
    "    - **Third-class passengers** had the lowest survival rate, particularly among males, indicating a strong influence of both class and gender on survival.\n",
    "\n",
    "#### Observations:\n",
    "- **Wealthier passengers** (first class) had better access to lifeboats, while passengers from lower classes (third class) had a much lower chance of survival.\n",
    "- **Gender** also played a significant role, as females were given priority during the rescue, leading to significantly higher survival rates among women.\n",
    "\n",
    "### 3.2 Influence of Fare on Survival\n",
    "- A boxplot of fare distribution across passenger classes and survival status shows that:\n",
    "    - Passengers who paid higher fares were more likely to survive.\n",
    "    - First-class passengers, who paid the highest fares, had the best survival rate, while third-class passengers with the lowest fares had the worst survival rate.\n",
    "\n",
    "#### Observations:\n",
    "- **Fare is a proxy for wealth**, and wealthier passengers, who had better accommodations, were more likely to have access to lifeboats, contributing to higher survival rates.\n",
    "\n",
    "### 3.3 Correlation Matrix for Social Inequality Features\n",
    "- A heatmap displaying the correlation between features related to social inequality (e.g., `Pclass`, `Fare`, `Age`, `Sex`, and `Title`) shows strong correlations:\n",
    "    - `Pclass` is strongly correlated with `Fare`, highlighting the fact that wealthier passengers traveled in higher classes.\n",
    "    - `Sex` and `Title_Mr`, `Title_Mrs`, and `Title_Miss` showed significant correlations with survival, reflecting the gender-based priority given during evacuation.\n",
    "\n",
    "#### Observations:\n",
    "- **Passenger class and fare** are key indicators of wealth, and wealth had a substantial impact on survival.\n",
    "- **Gender-based societal norms** during the time of the Titanic disaster are evident in the correlation between `Sex` and survival, as women had a higher survival rate.\n",
    "\n",
    "### 3.4 Influence of Family Size on Survival\n",
    "- A bar plot visualizing the survival rate by family size shows:\n",
    "    - **Smaller family sizes (1-3 members)** had higher survival rates compared to larger families.\n",
    "    - Passengers with very large families (more than 4 members) had significantly lower survival rates, possibly due to difficulties in evacuating larger groups.\n",
    "\n",
    "#### Observations:\n",
    "- Traveling alone or in small groups appears to have been an advantage during evacuation, while larger families faced more challenges, leading to lower survival rates.\n",
    "\n",
    "## 4. Model 1: Shallow Artificial Neural Network (ANN)\n",
    "\n",
    "### 4.1 Model Description\n",
    "We implemented a **Shallow ANN** with the following architecture:\n",
    "- **Input layer**: Corresponding to the number of input features.\n",
    "- **Hidden layer**: 6 neurons using the `tanh` activation function.\n",
    "- **Output layer**: Softmax function to handle multiclass classification.\n",
    "\n",
    "### 4.2 Results\n",
    "- After training the model for **30,000 epochs**, the **Shallow ANN** achieved an accuracy of **69.38%** (`np.float64(0.69375)`).\n",
    "\n",
    "### 4.3 Observations\n",
    "- While the shallow ANN performed decently, the accuracy was lower compared to the logistic regression model with feature engineering. This suggests that logistic regression is more effective for this dataset when `Pclass` is feature-engineered, potentially due to the smaller size and simplicity of the Titanic dataset.\n",
    "\n",
    "## 5. Model 2: Multiclass Logistic Regression\n",
    "\n",
    "### 5.1 Model Description\n",
    "We implemented a **Multiclass Logistic Regression** using softmax activation for multiclass classification. We focused on optimizing the model by:\n",
    "- Feature engineering the `Pclass` feature.\n",
    "- Fine-tuning hyperparameters such as the learning rate (`eta`) and the number of epochs.\n",
    "\n",
    "### 5.2 Results\n",
    "- Without feature engineering `Pclass` (using one-hot encoding), the logistic regression model achieved an accuracy of **58%**.\n",
    "- After feature engineering `Pclass`, the accuracy increased significantly to **95.6%**.\n",
    "\n",
    "### 5.3 Observations\n",
    "- The logistic regression model outperformed the shallow ANN when the `Pclass` feature was combined into a single column, highlighting the importance of proper feature engineering for this dataset.\n",
    "- The accuracy of **95.6%** demonstrates that logistic regression effectively models survival on the Titanic, especially with the engineered features.\n",
    "\n",
    "## 6. Hyperparameter Tuning and Performance Comparison\n",
    "\n",
    "### 6.1 Shallow ANN\n",
    "We performed hyperparameter tuning for the shallow ANN by experimenting with:\n",
    "- **Learning rates**: `1e-4`, `1e-3`, `1e-2`.\n",
    "- **Number of neurons** in the hidden layer: `3`, `6`, `10`.\n",
    "- **Number of epochs**: `5,000`, `10,000`, `30,000`.\n",
    "\n",
    "Despite extensive tuning, the shallow ANN achieved a maximum accuracy of **69.38%**, suggesting that more complex neural networks may be unnecessary for this dataset.\n",
    "\n",
    "### 6.2 Logistic Regression\n",
    "We tuned the logistic regression model and found that, after feature engineering, it achieved a near-perfect accuracy of **95.6%**. This highlights the effectiveness of logistic regression for this classification task, especially with properly engineered features like `Pclass`.\n",
    "\n",
    "## 7. Conclusion\n",
    "- The **Multiclass Logistic Regression** model performed best with an accuracy of **95.6%** after feature engineering the `Pclass` feature. This demonstrates the importance of preserving the ordinal relationship between passenger classes in the dataset.\n",
    "- The **Shallow ANN**, even with hyperparameter tuning, reached an accuracy of **69.38%**, which is lower compared to logistic regression.\n",
    "- **Feature engineering** and **hyperparameter tuning** played a critical role in improving model performance, particularly in the logistic regression model.\n",
    "\n",
    "## 8. Future Improvements\n",
    "- Consider using **deeper neural networks** for capturing more complex feature interactions.\n",
    "- Apply additional **feature engineering techniques**, such as binning continuous variables like `Age` and `Fare`, to further improve model performance.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
